{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d302a",
   "metadata": {},
   "source": [
    "### Understanding Store in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28880508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = r\"data\\toronto.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "# by default, we will split by pages with no text_splitter\n",
    "documents = loader.load_and_split(text_splitter=None)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e5a7f",
   "metadata": {},
   "source": [
    "### Unsing the PostgresByteStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c1001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_postgres import PGVector\n",
    "from database import COLLECTION_NAME, CONNECTION_STRING\n",
    "from utils.store import PostgresByteStore\n",
    "from langchain_postgres import PostgresSaver, PickleCheckpointSerializer\n",
    "from utils.custom_sql_record_manager import CustomSQLRecordManager\n",
    "from utils.index_with_ids import index_with_ids\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION_STRING,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "store = PostgresByteStore(CONNECTION_STRING, COLLECTION_NAME)\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    docstore=store, \n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# define record manager\n",
    "namespace = f\"pgvector/{COLLECTION_NAME}\"\n",
    "record_manager = CustomSQLRecordManager(\n",
    "    namespace, db_url=CONNECTION_STRING\n",
    ")\n",
    "record_manager.create_schema()\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6255a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import generate_reproducible_id_by_content, generate_reproducible_id_by_smartcontent\n",
    "\n",
    "# Add a reproducible unique doc_id to each document's metadata\n",
    "for position, doc in enumerate(documents):\n",
    "    doc.metadata[\"doc_id\"] = generate_reproducible_id_by_content(doc.page_content, doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9924d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of (doc_id, document) tuples from the documents\n",
    "doc_id_document_tuples = [(doc.metadata[\"doc_id\"], doc, doc.metadata[\"source\"]) for doc in documents]\n",
    "\n",
    "# Pass the list of tuples to retriever.docstore.conditional_mset\n",
    "parent_docs_operations = retriever.docstore.conditional_mset(doc_id_document_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_docs_operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cc3de",
   "metadata": {},
   "source": [
    "### Creating Smaller Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a80d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, String, LargeBinary, select, Table, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "separators = [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\"]\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter with fixed parameters\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    separators=separators\n",
    ")\n",
    "\n",
    "# List to store all sub-documents\n",
    "all_sub_docs = []\n",
    "\n",
    "# Database connection setup\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Define table structure\n",
    "metadata = MetaData()\n",
    "langchain_pg_embedding = Table(\n",
    "    'langchain_pg_embedding', metadata,\n",
    "    Column('id', String, primary_key=True),\n",
    "    Column('collection_id', String),\n",
    "    Column('embedding', LargeBinary),\n",
    "    Column('document', String),\n",
    "    Column('cmetadata', JSONB)\n",
    ")\n",
    "\n",
    "# Create a dictionary to map doc_id to documents\n",
    "documents_dict = {doc.metadata['doc_id']: doc for doc in documents}\n",
    "\n",
    "# Sort the parent documents operations to ensure deterministic processing order\n",
    "parent_docs_operations = sorted(parent_docs_operations, key=lambda x: x[0])\n",
    "\n",
    "# Iterate through the operations\n",
    "for doc_id, operation in parent_docs_operations:\n",
    "    if operation == 'SKIP':\n",
    "        # Fetch records from langchain_pg_embedding table for SKIP documents\n",
    "        query = select(\n",
    "            langchain_pg_embedding.c.id,\n",
    "            langchain_pg_embedding.c.collection_id,\n",
    "            langchain_pg_embedding.c.embedding,\n",
    "            langchain_pg_embedding.c.document,\n",
    "            langchain_pg_embedding.c.cmetadata\n",
    "        ).where(\n",
    "            (langchain_pg_embedding.c.cmetadata['doc_id'].astext == doc_id) &\n",
    "            (langchain_pg_embedding.c.cmetadata['type'].astext == 'smaller chunk')\n",
    "        ).order_by(langchain_pg_embedding.c.id)  # Ensure fixed order\n",
    "\n",
    "        result = session.execute(query).fetchall()\n",
    "\n",
    "        # Recreate sub-documents from fetched records\n",
    "        for row in result:\n",
    "            metadata = row.cmetadata\n",
    "            sub_doc_content = row.document\n",
    "            sub_doc = Document(page_content=sub_doc_content, metadata=metadata)\n",
    "            all_sub_docs.append(sub_doc)\n",
    "    elif doc_id in documents_dict:\n",
    "        # Retrieve the document from the provided documents for non-SKIP documents\n",
    "        doc = documents_dict[doc_id]\n",
    "        source = doc.metadata.get(\"source\")  # Retrieve the source from the document's metadata\n",
    "        sub_docs = child_text_splitter.split_documents([doc])\n",
    "        # Ensure fixed order for sub-documents\n",
    "        sub_docs = sorted(sub_docs, key=lambda x: x.page_content)\n",
    "        for sub_doc in sub_docs:\n",
    "            sub_doc.metadata[\"doc_id\"] = doc_id  # Assign the same doc_id to each sub-document\n",
    "            sub_doc.metadata[\"source\"] = f\"{source}(smaller chunk)\"  # Add the suffix to the source\n",
    "            sub_doc.metadata[\"type\"] = \"smaller chunk\"\n",
    "        all_sub_docs.extend(sub_docs)\n",
    "\n",
    "# Close the session after use\n",
    "session.close()\n",
    "\n",
    "# The resulting sub-documents\n",
    "\n",
    "# The resulting sub-documents\n",
    "all_sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e0cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = index_with_ids(all_sub_docs, record_manager, vectorstore, cleanup=\"incremental\",\n",
    "                                          source_id_key=\"source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b2e37",
   "metadata": {},
   "source": [
    "### Creating Summaries for Each Parent Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0407a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing and rephrasing text to maintain the original intent of the document. Rephrase the following text chunk in its original language, ensuring to preserve the original meaning and context, even if it comes from a chart or table: \\n\\n{element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Initialize the Language Model (LLM)\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# Define the summary chain\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42af4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, String, LargeBinary, select, Table, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# List to store all summary documents\n",
    "summary_docs = []\n",
    "\n",
    "# Database connection setup\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Define table structure\n",
    "metadata = MetaData()\n",
    "langchain_pg_embedding = Table(\n",
    "    'langchain_pg_embedding', metadata,\n",
    "    Column('id', String, primary_key=True),\n",
    "    Column('collection_id', String),\n",
    "    Column('embedding', LargeBinary),\n",
    "    Column('document', String),\n",
    "    Column('cmetadata', JSONB)\n",
    ")\n",
    "\n",
    "# Create a dictionary to map doc_id to documents\n",
    "documents_dict = {doc.metadata['doc_id']: doc for doc in documents}\n",
    "\n",
    "# Collect parent chunks and associated document IDs for documents that are not SKIP and exist in documents_dict\n",
    "non_skip_docs = [(documents_dict[doc_id], doc_id) for doc_id, operation in parent_docs_operations if operation != 'SKIP' and doc_id in documents_dict]\n",
    "skip_doc_ids = [doc_id for doc_id, operation in parent_docs_operations if operation == 'SKIP']\n",
    "\n",
    "# Generate summaries for the parent chunks that are not SKIP\n",
    "parent_chunk = [doc.page_content for doc, _ in non_skip_docs]\n",
    "text_summaries = summarize_chain.batch(parent_chunk, {\"max_concurrency\": 5})\n",
    "text_summaries_iter = iter(text_summaries)\n",
    "\n",
    "# Dictionary to store summaries temporarily\n",
    "temp_summary_docs = {}\n",
    "\n",
    "# Process non-SKIP documents and store their summaries\n",
    "for doc, doc_id in non_skip_docs:\n",
    "    source = doc.metadata.get(\"source\")\n",
    "    page = doc.metadata.get(\"page\")\n",
    "    summary_content = next(text_summaries_iter)\n",
    "    summary_doc = Document(page_content=summary_content, metadata={\n",
    "        \"doc_id\": doc_id,\n",
    "        \"source\": f\"{source}(summary)\",\n",
    "        \"page\": page,\n",
    "        \"type\": \"summary\"\n",
    "    })\n",
    "    temp_summary_docs[doc_id] = summary_doc\n",
    "\n",
    "# Process SKIP documents and store their summaries\n",
    "for doc_id in skip_doc_ids:\n",
    "    query = select(\n",
    "        langchain_pg_embedding.c.id,\n",
    "        langchain_pg_embedding.c.collection_id,\n",
    "        langchain_pg_embedding.c.embedding,\n",
    "        langchain_pg_embedding.c.document,\n",
    "        langchain_pg_embedding.c.cmetadata\n",
    "    ).where(\n",
    "        (langchain_pg_embedding.c.cmetadata['doc_id'].astext == doc_id) &\n",
    "        (langchain_pg_embedding.c.cmetadata['type'].astext == 'summary')\n",
    "    )\n",
    "\n",
    "    result = session.execute(query).fetchall()\n",
    "\n",
    "    if not result:\n",
    "        print(f\"No result found for SKIP doc_id {doc_id}\")\n",
    "    else:\n",
    "        for row in result:\n",
    "            metadata = row.cmetadata\n",
    "            summary_content = row.document\n",
    "            summary_doc = Document(page_content=summary_content, metadata=metadata)\n",
    "            temp_summary_docs[doc_id] = summary_doc\n",
    "\n",
    "# Combine the summaries into the final summary_docs list\n",
    "for doc in documents:\n",
    "    doc_id = doc.metadata['doc_id']\n",
    "    if doc_id in temp_summary_docs:\n",
    "        summary_docs.append(temp_summary_docs[doc_id])\n",
    "    else:\n",
    "        # Handle the case where no summary was found or generated\n",
    "        print(f\"No summary found for document ID {doc_id}\")\n",
    "\n",
    "# Close the session after use\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b393f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9317f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = index_with_ids(summary_docs, record_manager, vectorstore, cleanup=\"incremental\",\n",
    "                                          source_id_key=\"source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf4b08",
   "metadata": {},
   "source": [
    "### Generating Hypothetical Questions for Each Parent Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec26a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"hypothetical_questions\",\n",
    "        \"description\": \"Generate hypothetical questions\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"questions\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"questions\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e853c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "question_chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # Only asking for 5 hypothetical questions, but this could be adjusted\n",
    "    | ChatPromptTemplate.from_template(\n",
    "#         \"\"\"Generate a list of exactly 5 hypothetical questions that the below document could be used to answer, in the original language of the text:\\n\\n{doc}\n",
    "# Separate each question with a comma (,)\n",
    "#         \"\"\"\n",
    "\"\"\"Générez une liste de 5 questions hypothétiques que le document ci-dessous pourrait être utilisé pour répondre par a des questions sur le document, dans la langue originale du texte. Le document est extrait d'un PPTX ou PDF et peut nécessiter l'analyse du contexte et de la signification. Si le texte est trop court pour générer 5 questions, fournissez autant de questions que le texte le permet.\n",
    "\\nLe texte généré sera utilisé pour effectuer des recherches de similarité dans une base de données vectorielle.\n",
    "\\n\\n{doc}\\n\\n\n",
    "\n",
    "Séparez chaque question par une virgule (,).\n",
    "\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(max_retries=0, model=\"gpt-4o\").bind(\n",
    "        functions=functions, function_call={\"name\": \"hypothetical_questions\"}\n",
    "    )\n",
    "    | JsonKeyOutputFunctionsParser(key_name=\"questions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, String, LargeBinary, select, Table, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# List to store all question documents\n",
    "question_docs = []\n",
    "\n",
    "# Database connection setup\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Define table structure\n",
    "metadata = MetaData()\n",
    "langchain_pg_embedding = Table(\n",
    "    'langchain_pg_embedding', metadata,\n",
    "    Column('id', String, primary_key=True),\n",
    "    Column('collection_id', String),\n",
    "    Column('embedding', LargeBinary),\n",
    "    Column('document', String),\n",
    "    Column('cmetadata', JSONB)\n",
    ")\n",
    "\n",
    "# Create a dictionary to map doc_id to documents\n",
    "documents_dict = {doc.metadata['doc_id']: doc for doc in documents}\n",
    "\n",
    "# Separate non-SKIP and SKIP document IDs\n",
    "non_skip_docs = [(documents_dict[doc_id], doc_id) for doc_id, operation in parent_docs_operations if operation != 'SKIP' and doc_id in documents_dict]\n",
    "skip_doc_ids = [doc_id for doc_id, operation in parent_docs_operations if operation == 'SKIP']\n",
    "\n",
    "# Generate hypothetical questions for the parent documents that are not SKIP\n",
    "parent_documents = [doc for doc, _ in non_skip_docs]\n",
    "\n",
    "# Debugging: Check the number of parent documents\n",
    "print(f\"Number of non-SKIP documents: {len(parent_documents)}\")\n",
    "\n",
    "# Check if parent_documents is not empty\n",
    "if parent_documents:\n",
    "    hypothetical_questions = question_chain.batch(parent_documents, {\"max_concurrency\": 5})\n",
    "    hypothetical_questions_iter = iter(hypothetical_questions)\n",
    "else:\n",
    "    hypothetical_questions_iter = iter([])\n",
    "\n",
    "# Dictionary to store questions temporarily\n",
    "temp_question_docs = {}\n",
    "\n",
    "# Process non-SKIP documents and store their questions\n",
    "for doc, doc_id in non_skip_docs:\n",
    "    source = doc.metadata.get(\"source\")\n",
    "    page = doc.metadata.get(\"page\")\n",
    "\n",
    "    try:\n",
    "        question_list = next(hypothetical_questions_iter)\n",
    "    except StopIteration:\n",
    "        print(f\"No more questions available for doc_id {doc_id}\")\n",
    "        continue\n",
    "\n",
    "    for question_content in question_list:\n",
    "        question_doc = Document(page_content=question_content, metadata={\n",
    "            \"doc_id\": doc_id,\n",
    "            \"source\": f\"{source}(question)\",\n",
    "            \"page\": page,\n",
    "            \"type\": \"question\"\n",
    "        })\n",
    "        if doc_id not in temp_question_docs:\n",
    "            temp_question_docs[doc_id] = []\n",
    "        temp_question_docs[doc_id].append(question_doc)\n",
    "\n",
    "# Process SKIP documents and store their questions\n",
    "for doc_id in skip_doc_ids:\n",
    "    query = select(\n",
    "        langchain_pg_embedding.c.id,\n",
    "        langchain_pg_embedding.c.collection_id,\n",
    "        langchain_pg_embedding.c.embedding,\n",
    "        langchain_pg_embedding.c.document,\n",
    "        langchain_pg_embedding.c.cmetadata\n",
    "    ).where(\n",
    "        (langchain_pg_embedding.c.cmetadata['doc_id'].astext == doc_id) &\n",
    "        (langchain_pg_embedding.c.cmetadata['type'].astext == 'question')\n",
    "    )\n",
    "\n",
    "    result = session.execute(query).fetchall()\n",
    "\n",
    "    if result:\n",
    "        questions = []\n",
    "        for row in result:\n",
    "            metadata = row.cmetadata\n",
    "            question_content = row.document\n",
    "            question_doc = Document(page_content=question_content, metadata=metadata)\n",
    "            questions.append(question_doc)\n",
    "        temp_question_docs[doc_id] = questions\n",
    "\n",
    "# Combine the questions into the final question_docs list\n",
    "for doc in documents:\n",
    "    doc_id = doc.metadata['doc_id']\n",
    "    if doc_id in temp_question_docs:\n",
    "        question_docs.extend(temp_question_docs[doc_id])\n",
    "\n",
    "# Close the session after use\n",
    "session.close()\n",
    "\n",
    "# The resulting question documents\n",
    "question_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27139daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents = []\n",
    "for doc in question_docs:\n",
    "    page_contents.append(doc.page_content)\n",
    "\n",
    "print(page_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc293300",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = index_with_ids(question_docs, record_manager, vectorstore, cleanup=\"incremental\",\n",
    "                                          source_id_key=\"source\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbdbcd4",
   "metadata": {},
   "source": [
    "### Creating an LCEL Chain and Testing the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Où doit-on donner son avis ?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef47d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Quelle est l'adresse courriel mentionnée dans le document ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
