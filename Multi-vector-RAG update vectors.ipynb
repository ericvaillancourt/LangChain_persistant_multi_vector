{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0a71bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d302a",
   "metadata": {},
   "source": [
    "### Understanding Store in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28880508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Things to Do in Toronto \\nPage 1: Introduction \\nToronto, the capital of Ontario, is the largest city in Canada and a dynamic, cosmopolitan \\nhub. Known for its towering skyline, bustling waterfront, and numerous cultural attractions, \\nToronto o Ưers a wealth of experiences for every visitor.  \\nKey Attractions:  \\n\\uf0b7 CN Tower:  This iconic symbol of Toronto o Ưers panoramic views of the city. Don’t \\nmiss the glass ﬂoor and the revolving restaurant at the top. \\n\\uf0b7 Royal Ontario Museum (ROM):  Canada’s largest museum of world cultures and \\nnatural history is a must-visit. \\n\\uf0b7 Toronto Islands:  A group of small islands located just o Ư the city’s shore, o Ưering \\nbeautiful beaches, picnic spots, and bike rentals.', metadata={'source': 'data\\\\toronto.pdf', 'page': 0}),\n",
       " Document(page_content='Page 2: Cultural Experiences \\nToronto is a melting pot of cultures, and this is reﬂected in its neighborhoods and festivals. \\nNeighborhoods:  \\n\\uf0b7 Chinatown:  One of North America’s largest Chinatowns, known for its vibrant food \\nscene. \\n\\uf0b7 Kensington Market:  A bohemian neighborhood o Ưering vintage shops, eclectic \\nboutiques, and international food stalls. \\n\\uf0b7 Distillery District:  Known for its well-preserved Victorian Industrial architecture, it’s \\nnow home to boutiques, art galleries, and performance spaces. \\nFestivals:  \\n\\uf0b7 Caribana:  A festival celebrating Caribbean culture and traditions, held in summer. \\n\\uf0b7 Toronto International Film Festival (TIFF):  One of the most prestigious ﬁlm \\nfestivals in the world, held annually in September.', metadata={'source': 'data\\\\toronto.pdf', 'page': 1}),\n",
       " Document(page_content='Page 3: Outdoor Activities \\nToronto o Ưers numerous opportunities for outdoor activities.  \\n\\uf0b7 High Park:  Toronto’s largest public park featuring many hiking trails, sports facilities, \\na beautiful lakefront, a zoo, and several playgrounds. \\n\\uf0b7 Toronto Zoo:  Home to over 5,000 animals representing over 500 species. \\n\\uf0b7 Ripley’s Aquarium of Canada:  Located at the base of the CN Tower, this enormous \\naquarium is one of the city’s newest top attractions.', metadata={'source': 'data\\\\toronto.pdf', 'page': 2}),\n",
       " Document(page_content='Page 4: Food and Nightlife \\nToronto’s food scene is as diverse as its population. \\n\\uf0b7 St. Lawrence Market:  Named the world’s best food market by National Geographic \\nin 2012, this is a must-visit for foodies. \\n\\uf0b7 Nightlife:  Toronto has a vibrant nightlife with a plethora of bars, nightclubs, and live \\nmusic venues. The Entertainment District is known for its nightclubs and theaters. \\nIn conclusion, whether you’re a lover of art and culture, outdoor activities, food, or just \\nlooking to have a good time, Toronto has something for everyone.', metadata={'source': 'data\\\\toronto.pdf', 'page': 3})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = r\"data\\toronto.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "# by default, we will split by pages with no text_splitter\n",
    "documents = loader.load_and_split(text_splitter=None)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e5a7f",
   "metadata": {},
   "source": [
    "### Unsing the PostgresByteStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833c1001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiVectorRetriever(vectorstore=<langchain_postgres.vectorstores.PGVector object at 0x0000025FEF02AA90>, docstore=<utils.store.PostgresByteStore object at 0x0000025FEF0F7350>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_postgres import PGVector\n",
    "from database import COLLECTION_NAME, CONNECTION_STRING\n",
    "from utils.store import PostgresByteStore\n",
    "from langchain_postgres import PostgresSaver, PickleCheckpointSerializer\n",
    "from utils.custom_sql_record_manager import CustomSQLRecordManager\n",
    "from utils.index_with_ids import index_with_ids\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION_STRING,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "store = PostgresByteStore(CONNECTION_STRING, COLLECTION_NAME)\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    docstore=store, \n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# define record manager\n",
    "namespace = f\"pgvector/{COLLECTION_NAME}\"\n",
    "record_manager = CustomSQLRecordManager(\n",
    "    namespace, db_url=CONNECTION_STRING\n",
    ")\n",
    "record_manager.create_schema()\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6255a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import generate_reproducible_id_by_content\n",
    "\n",
    "# Add a reproducible unique doc_id to each document's metadata\n",
    "for position, doc in enumerate(documents):\n",
    "    doc.metadata[\"doc_id\"] = generate_reproducible_id_by_content(doc.page_content, doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e631a833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Things to Do in Toronto \\nPage 1: Introduction \\nToronto, the capital of Ontario, is the largest city in Canada and a dynamic, cosmopolitan \\nhub. Known for its towering skyline, bustling waterfront, and numerous cultural attractions, \\nToronto o Ưers a wealth of experiences for every visitor.  \\nKey Attractions:  \\n\\uf0b7 CN Tower:  This iconic symbol of Toronto o Ưers panoramic views of the city. Don’t \\nmiss the glass ﬂoor and the revolving restaurant at the top. \\n\\uf0b7 Royal Ontario Museum (ROM):  Canada’s largest museum of world cultures and \\nnatural history is a must-visit. \\n\\uf0b7 Toronto Islands:  A group of small islands located just o Ư the city’s shore, o Ưering \\nbeautiful beaches, picnic spots, and bike rentals.', metadata={'source': 'data\\\\toronto.pdf', 'page': 0, 'doc_id': '70a01880-8f42-5b40-9fb0-85852ecc0f1d'}),\n",
       " Document(page_content='Page 2: Cultural Experiences \\nToronto is a melting pot of cultures, and this is reﬂected in its neighborhoods and festivals. \\nNeighborhoods:  \\n\\uf0b7 Chinatown:  One of North America’s largest Chinatowns, known for its vibrant food \\nscene. \\n\\uf0b7 Kensington Market:  A bohemian neighborhood o Ưering vintage shops, eclectic \\nboutiques, and international food stalls. \\n\\uf0b7 Distillery District:  Known for its well-preserved Victorian Industrial architecture, it’s \\nnow home to boutiques, art galleries, and performance spaces. \\nFestivals:  \\n\\uf0b7 Caribana:  A festival celebrating Caribbean culture and traditions, held in summer. \\n\\uf0b7 Toronto International Film Festival (TIFF):  One of the most prestigious ﬁlm \\nfestivals in the world, held annually in September.', metadata={'source': 'data\\\\toronto.pdf', 'page': 1, 'doc_id': '4d722603-1c85-56ab-82f2-2d4dfdd3eb68'}),\n",
       " Document(page_content='Page 3: Outdoor Activities \\nToronto o Ưers numerous opportunities for outdoor activities.  \\n\\uf0b7 High Park:  Toronto’s largest public park featuring many hiking trails, sports facilities, \\na beautiful lakefront, a zoo, and several playgrounds. \\n\\uf0b7 Toronto Zoo:  Home to over 5,000 animals representing over 500 species. \\n\\uf0b7 Ripley’s Aquarium of Canada:  Located at the base of the CN Tower, this enormous \\naquarium is one of the city’s newest top attractions.', metadata={'source': 'data\\\\toronto.pdf', 'page': 2, 'doc_id': '80e1944b-044e-5dc8-899f-7a941f1fa08b'}),\n",
       " Document(page_content='Page 4: Food and Nightlife \\nToronto’s food scene is as diverse as its population. \\n\\uf0b7 St. Lawrence Market:  Named the world’s best food market by National Geographic \\nin 2012, this is a must-visit for foodies. \\n\\uf0b7 Nightlife:  Toronto has a vibrant nightlife with a plethora of bars, nightclubs, and live \\nmusic venues. The Entertainment District is known for its nightclubs and theaters. \\nIn conclusion, whether you’re a lover of art and culture, outdoor activities, food, or just \\nlooking to have a good time, Toronto has something for everyone.', metadata={'source': 'data\\\\toronto.pdf', 'page': 3, 'doc_id': 'a9cace25-2615-59b1-9669-ccd6656ac767'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9924d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of (doc_id, document) tuples from the documents\n",
    "doc_id_document_tuples = [(doc.metadata[\"doc_id\"], doc, doc.metadata[\"source\"]) for doc in documents]\n",
    "\n",
    "# Pass the list of tuples to retriever.docstore.conditional_mset\n",
    "parent_docs_operations = retriever.docstore.conditional_mset(doc_id_document_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e59b583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('756d43c5-71a0-5ce9-8c77-4f23e8c7721f', 'DEL'),\n",
       " ('70a01880-8f42-5b40-9fb0-85852ecc0f1d', 'INS'),\n",
       " ('4d722603-1c85-56ab-82f2-2d4dfdd3eb68', 'SKIP'),\n",
       " ('80e1944b-044e-5dc8-899f-7a941f1fa08b', 'SKIP'),\n",
       " ('a9cace25-2615-59b1-9669-ccd6656ac767', 'SKIP')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_docs_operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cc3de",
   "metadata": {},
   "source": [
    "### Creating Smaller Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f82443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, String, LargeBinary, select, Table, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from langchain.schema.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "separators = [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\"]\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter with fixed parameters\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    separators=separators\n",
    ")\n",
    "\n",
    "# List to store all sub-documents\n",
    "all_sub_docs = []\n",
    "\n",
    "# Database connection setup\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Define table structure\n",
    "metadata = MetaData()\n",
    "langchain_pg_embedding = Table(\n",
    "    'langchain_pg_embedding', metadata,\n",
    "    Column('id', String, primary_key=True),\n",
    "    Column('collection_id', String),\n",
    "    Column('embedding', LargeBinary),\n",
    "    Column('document', String),\n",
    "    Column('cmetadata', JSONB)\n",
    ")\n",
    "\n",
    "# Sort the parent documents operations to ensure deterministic processing order\n",
    "parent_docs_operations = sorted(parent_docs_operations, key=lambda x: x[0])\n",
    "\n",
    "# Iterate through the operations\n",
    "for doc_id, operation in parent_docs_operations:\n",
    "    if operation == 'SKIP':\n",
    "        # Fetch records from langchain_pg_embedding table for SKIP documents\n",
    "        query = select(\n",
    "            langchain_pg_embedding.c.id,\n",
    "            langchain_pg_embedding.c.collection_id,\n",
    "            langchain_pg_embedding.c.embedding,\n",
    "            langchain_pg_embedding.c.document,\n",
    "            langchain_pg_embedding.c.cmetadata\n",
    "        ).where(\n",
    "            (langchain_pg_embedding.c.cmetadata['doc_id'].astext == doc_id) &\n",
    "            (langchain_pg_embedding.c.cmetadata['type'].astext == 'smaller chunk')\n",
    "        ).order_by(langchain_pg_embedding.c.id)  # Ensure fixed order\n",
    "        \n",
    "        result = session.execute(query).fetchall()\n",
    "        \n",
    "        # Recreate sub-documents from fetched records\n",
    "        for row in result:\n",
    "            metadata = row.cmetadata\n",
    "            sub_doc_content = row.document\n",
    "            sub_doc = Document(page_content=sub_doc_content, metadata=metadata)\n",
    "            all_sub_docs.append(sub_doc)\n",
    "    else:\n",
    "        # Retrieve the document from the docstore for non-SKIP documents\n",
    "        doc = retriever.docstore.get(doc_id)\n",
    "        if doc:\n",
    "            source = doc.metadata.get(\"source\")  # Retrieve the source from the document's metadata\n",
    "            sub_docs = child_text_splitter.split_documents([doc])\n",
    "            # Ensure fixed order for sub-documents\n",
    "            sub_docs = sorted(sub_docs, key=lambda x: x.page_content)\n",
    "            for sub_doc in sub_docs:\n",
    "                sub_doc.metadata[\"doc_id\"] = doc_id  # Assign the same doc_id to each sub-document\n",
    "                sub_doc.metadata[\"source\"] = f\"{source}(smaller chunk)\"  # Add the suffix to the source\n",
    "                sub_doc.metadata[\"type\"] = \"smaller chunk\"\n",
    "            all_sub_docs.extend(sub_docs)\n",
    "\n",
    "# Close the session after use\n",
    "session.close()\n",
    "\n",
    "# The resulting sub-documents\n",
    "all_sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e0cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = index_with_ids(all_sub_docs, record_manager, vectorstore, cleanup=\"incremental\",\n",
    "                                          source_id_key=\"source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b2e37",
   "metadata": {},
   "source": [
    "### Creating Summaries for Each Parent Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0407a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing and rephrasing text to maintain the original intent of the document. Rephrase the following text chunk in its original language, ensuring to preserve the original meaning and context, even if it comes from a chart or table: \\n\\n{element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Initialize the Language Model (LLM)\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# Define the summary chain\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42af4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, String, LargeBinary, select, Table, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# List to store all summary documents\n",
    "summary_docs = []\n",
    "\n",
    "# Database connection setup\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Define table structure\n",
    "metadata = MetaData()\n",
    "langchain_pg_embedding = Table(\n",
    "    'langchain_pg_embedding', metadata,\n",
    "    Column('id', String, primary_key=True),\n",
    "    Column('collection_id', String),\n",
    "    Column('embedding', LargeBinary),\n",
    "    Column('document', String),\n",
    "    Column('cmetadata', JSONB)\n",
    ")\n",
    "\n",
    "# Create a dictionary to map doc_id to documents\n",
    "documents_dict = {doc.metadata['doc_id']: doc for doc in documents}\n",
    "\n",
    "# Collect parent chunks and associated document IDs for documents that are not SKIP and exist in documents_dict\n",
    "non_skip_docs = [(documents_dict[doc_id], doc_id) for doc_id, operation in parent_docs_operations if operation != 'SKIP' and doc_id in documents_dict]\n",
    "skip_doc_ids = [doc_id for doc_id, operation in parent_docs_operations if operation == 'SKIP']\n",
    "\n",
    "\n",
    "# Generate summaries for the parent chunks that are not SKIP\n",
    "parent_chunk = [doc.page_content for doc, _ in non_skip_docs]\n",
    "text_summaries = summarize_chain.batch(parent_chunk, {\"max_concurrency\": 5})\n",
    "text_summaries_iter = iter(text_summaries)\n",
    "\n",
    "# Dictionary to store summaries temporarily\n",
    "temp_summary_docs = {}\n",
    "\n",
    "# Process non-SKIP documents and store their summaries\n",
    "for doc, doc_id in non_skip_docs:\n",
    "    source = doc.metadata.get(\"source\")\n",
    "    page = doc.metadata.get(\"page\")\n",
    "    summary_content = next(text_summaries_iter)\n",
    "    summary_doc = Document(page_content=summary_content, metadata={\n",
    "        \"doc_id\": doc_id,\n",
    "        \"source\": f\"{source}(summary)\",\n",
    "        \"page\": page,\n",
    "        \"type\": \"summary\"\n",
    "    })\n",
    "    temp_summary_docs[doc_id] = summary_doc\n",
    "\n",
    "\n",
    "# Process SKIP documents and store their summaries\n",
    "for doc_id in skip_doc_ids:\n",
    "    query = select(\n",
    "        langchain_pg_embedding.c.id,\n",
    "        langchain_pg_embedding.c.collection_id,\n",
    "        langchain_pg_embedding.c.embedding,\n",
    "        langchain_pg_embedding.c.document,\n",
    "        langchain_pg_embedding.c.cmetadata\n",
    "    ).where(\n",
    "        (langchain_pg_embedding.c.cmetadata['doc_id'].astext == doc_id) &\n",
    "        (langchain_pg_embedding.c.cmetadata['type'].astext == 'summary')\n",
    "    )\n",
    "\n",
    "\n",
    "    result = session.execute(query).fetchall()\n",
    "\n",
    "    if not result:\n",
    "        print(f\"No result found for SKIP doc_id {doc_id}\")\n",
    "    else:\n",
    "        for row in result:\n",
    "            metadata = row.cmetadata\n",
    "            summary_content = row.document\n",
    "            summary_doc = Document(page_content=summary_content, metadata=metadata)\n",
    "            temp_summary_docs[doc_id] = summary_doc\n",
    "\n",
    "\n",
    "# Combine the summaries into the final summary_docs list\n",
    "for doc in documents:\n",
    "    doc_id = doc.metadata['doc_id']\n",
    "    if doc_id in temp_summary_docs:\n",
    "        summary_docs.append(temp_summary_docs[doc_id])\n",
    "    else:\n",
    "        # Handle the case where no summary was found or generated\n",
    "        print(f\"No summary found for document ID {doc_id}\")\n",
    "\n",
    "# Close the session after use\n",
    "session.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b393f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9317f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = index_with_ids(summary_docs, record_manager, vectorstore, cleanup=\"incremental\",\n",
    "                                          source_id_key=\"source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf4b08",
   "metadata": {},
   "source": [
    "### Generating Hypothetical Questions for Each Parent Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec26a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"hypothetical_questions\",\n",
    "        \"description\": \"Generate hypothetical questions\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"questions\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"questions\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e853c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "question_chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # Only asking for 5 hypothetical questions, but this could be adjusted\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"\"\"Generate a list of exactly 5 hypothetical questions that the below document could be used to answer, in the original language of the text:\\n\\n{doc}\n",
    "Separate each question with a comma (,)\n",
    "        \"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(max_retries=0, model=\"gpt-4o\").bind(\n",
    "        functions=functions, function_call={\"name\": \"hypothetical_questions\"}\n",
    "    )\n",
    "    | JsonKeyOutputFunctionsParser(key_name=\"questions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, String, LargeBinary, select, Table, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# List to store all question documents\n",
    "question_docs = []\n",
    "\n",
    "# Database connection setup\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Define table structure\n",
    "metadata = MetaData()\n",
    "langchain_pg_embedding = Table(\n",
    "    'langchain_pg_embedding', metadata,\n",
    "    Column('id', String, primary_key=True),\n",
    "    Column('collection_id', String),\n",
    "    Column('embedding', LargeBinary),\n",
    "    Column('document', String),\n",
    "    Column('cmetadata', JSONB)\n",
    ")\n",
    "\n",
    "# Create a dictionary to map doc_id to documents\n",
    "documents_dict = {doc.metadata['doc_id']: doc for doc in documents}\n",
    "\n",
    "# Separate non-SKIP and SKIP document IDs\n",
    "non_skip_docs = [(documents_dict[doc_id], doc_id) for doc_id, operation in parent_docs_operations if operation != 'SKIP' and doc_id in documents_dict]\n",
    "skip_doc_ids = [doc_id for doc_id, operation in parent_docs_operations if operation == 'SKIP']\n",
    "\n",
    "# Generate hypothetical questions for the parent documents that are not SKIP\n",
    "parent_documents = [doc for doc, _ in non_skip_docs]\n",
    "hypothetical_questions = question_chain.batch(parent_documents, {\"max_concurrency\": 5})\n",
    "hypothetical_questions_iter = iter(hypothetical_questions)\n",
    "\n",
    "# Dictionary to store questions temporarily\n",
    "temp_question_docs = {}\n",
    "\n",
    "# Process non-SKIP documents and store their questions\n",
    "for doc, doc_id in non_skip_docs:\n",
    "    source = doc.metadata.get(\"source\")\n",
    "    page = doc.metadata.get(\"page\")\n",
    "    question_list = next(hypothetical_questions_iter)\n",
    "    \n",
    "    # Ensure there are exactly 5 questions for each document\n",
    "    if len(question_list) < 5:\n",
    "        question_list = question_list + [\"\"] * (5 - len(question_list))  # Pad with empty strings if fewer than 5\n",
    "    \n",
    "    for question_content in question_list[:5]:\n",
    "        question_doc = Document(page_content=question_content, metadata={\n",
    "            \"doc_id\": doc_id,\n",
    "            \"source\": f\"{source}(question)\",\n",
    "            \"page\": page,\n",
    "            \"type\": \"question\"\n",
    "        })\n",
    "        if doc_id not in temp_question_docs:\n",
    "            temp_question_docs[doc_id] = []\n",
    "        temp_question_docs[doc_id].append(question_doc)\n",
    "\n",
    "# Process SKIP documents and store their questions\n",
    "for doc_id in skip_doc_ids:\n",
    "    query = select(\n",
    "        langchain_pg_embedding.c.id,\n",
    "        langchain_pg_embedding.c.collection_id,\n",
    "        langchain_pg_embedding.c.embedding,\n",
    "        langchain_pg_embedding.c.document,\n",
    "        langchain_pg_embedding.c.cmetadata\n",
    "    ).where(\n",
    "        (langchain_pg_embedding.c.cmetadata['doc_id'].astext == doc_id) &\n",
    "        (langchain_pg_embedding.c.cmetadata['type'].astext == 'question')\n",
    "    )\n",
    "\n",
    "    result = session.execute(query).fetchall()\n",
    "\n",
    "    if result:\n",
    "        questions = []\n",
    "        for row in result:\n",
    "            metadata = row.cmetadata\n",
    "            question_content = row.document\n",
    "            question_doc = Document(page_content=question_content, metadata=metadata)\n",
    "            questions.append(question_doc)\n",
    "        \n",
    "        # Ensure there are exactly 5 questions for each document\n",
    "        if len(questions) < 5:\n",
    "            questions = questions + [Document(page_content=\"\", metadata={\n",
    "                \"doc_id\": doc_id,\n",
    "                \"source\": f\"{documents_dict[doc_id].metadata.get('source')}(question)\",\n",
    "                \"page\": documents_dict[doc_id].metadata.get(\"page\"),\n",
    "                \"type\": \"question\"\n",
    "            }) for _ in range(5 - len(questions))]  # Pad with empty documents if fewer than 5\n",
    "        \n",
    "        temp_question_docs[doc_id] = questions[:5]\n",
    "\n",
    "# Combine the questions into the final question_docs list\n",
    "for doc in documents:\n",
    "    doc_id = doc.metadata['doc_id']\n",
    "    if doc_id in temp_question_docs:\n",
    "        question_docs.extend(temp_question_docs[doc_id])\n",
    "\n",
    "# Close the session after use\n",
    "session.close()\n",
    "\n",
    "# The resulting question documents\n",
    "question_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc293300",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = index_with_ids(question_docs, record_manager, vectorstore, cleanup=\"incremental\",\n",
    "                                          source_id_key=\"source\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbdbcd4",
   "metadata": {},
   "source": [
    "### Creating an LCEL Chain and Testing the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Où doit-on donner son avis ?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef47d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Quelle est l'adresse courriel mentionnée dans le document ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
